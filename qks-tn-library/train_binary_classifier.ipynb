{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import sys, os, time\n",
    "gen_fn_dir = os.path.abspath('./common_functions')\n",
    "sys.path.append(gen_fn_dir)\n",
    "\n",
    "import qks_tn as qksTN\n",
    "from general_functions import deskewAll\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "import keras\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import tensornetwork as tn\n",
    "tn.set_default_backend('tensorflow')\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading MNIST Data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Selecting out (3,5)-MNIST\n",
    "X_train = X_train[(y_train==3) | (y_train==5)]\n",
    "y_train = y_train[(y_train==3) | (y_train==5)]\n",
    "X_test = X_test[(y_test==3) | (y_test==5)]\n",
    "y_test = y_test[(y_test==3) | (y_test==5)]\n",
    "\n",
    "y_train[y_train==3] = 0\n",
    "y_train[y_train==5] = 1\n",
    "y_test[y_test==3] = 0\n",
    "y_test[y_test==5] = 1\n",
    "\n",
    "# Reshaping (2D --> 1D) and rescaling (0-255 --> 0-1) \n",
    "X_train = X_train.reshape((X_train.shape[0],-1))\n",
    "X_test = X_test.reshape((X_test.shape[0],-1))\n",
    "X_train, X_test = X_train/255, X_test/255\n",
    "\n",
    "# Reducing the precision of the data\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Deskewing the data\n",
    "X_train = deskewAll(X_train)\n",
    "X_test = deskewAll(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters for QKS and TTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for QKS\n",
    "nepisodes = 128\n",
    "p = 784\n",
    "q = 1\n",
    "r = int(p/q)\n",
    "\n",
    "sigma = 0.125\n",
    "    \n",
    "# Parameters for TTN\n",
    "chi = 4\n",
    "nqubits = int(np.log2(chi))\n",
    "nlayers = int(np.log2(nepisodes/nqubits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the contraction path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting contraction path\n",
    "QKS = qksTN.FeatureEncodingLayer(nepisodes,chi,p,sigma)\n",
    "rho_test = QKS.call(X_test).numpy()\n",
    "\n",
    "uni_array = tf.constant(np.zeros(([chi]*4+[nlayers])),dtype='complex64')\n",
    "obs = []\n",
    "for j in range(int(nepisodes/2)):\n",
    "    obs.append(rho_test[0,:,:,j])\n",
    "\n",
    "nodes_set, edge_order = qksTN.construct_dttn(chi,uni_array,obs,nepisodes)\n",
    "result, path = qksTN.greedy(nodes_set,output_edge_order=edge_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model and optimizer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " feature_encoding_layer_1 (F  (None, 4, 4, 64)         100480    \n",
      " eatureEncodingLayer)                                            \n",
      "                                                                 \n",
      " tn_layer (TNLayer)          (None, 16)                16065     \n",
      "                                                                 \n",
      " const_mul_binary (ConstMulB  (None, 2)                0         \n",
      " inary)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 116,545\n",
      "Trainable params: 116,545\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting training parameters\n",
    "start_epoch = 0\n",
    "nepochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "# Defining the model architecture\n",
    "tn_model = tf.keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(p,)),\n",
    "        qksTN.FeatureEncodingLayer(nepisodes,chi,p,sigma),\n",
    "        qksTN.TNLayer(chi,nlayers,path),\n",
    "        qksTN.ConstMulBinary()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scheduling learning rate and weight decay for AdamW optimizer\n",
    "lr_schedule = tf.optimizers.schedules.CosineDecayRestarts(1e-3,y_train.shape[0]//batch_size)\n",
    "wd_schedule = tf.optimizers.schedules.CosineDecayRestarts(4e-4,y_train.shape[0]//batch_size)\n",
    "\n",
    "# Compiling the model\n",
    "tn_model.compile(loss='sparse_categorical_crossentropy', optimizer=AdamW(learning_rate=lr_schedule, weight_decay=wd_schedule), metrics=['accuracy'])\n",
    "tn_model.summary()\n",
    "\n",
    "# Defining how batches are drawn from the datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=y_train.shape[0]).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nxkod\\qks-tn\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential/tn_layer/loop_body/GatherV2/pfor/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential/tn_layer/loop_body/GatherV2/pfor/Reshape:0\", shape=(None, 4, 4, 64), dtype=complex64), dense_shape=Tensor(\"gradient_tape/sequential/tn_layer/loop_body/GatherV2/pfor/Cast:0\", shape=(4,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361/361 [==============================] - 228s 215ms/step - loss: 0.3208 - accuracy: 0.9627\n",
      "Epoch 2/15\n",
      "361/361 [==============================] - 85s 235ms/step - loss: 0.0782 - accuracy: 0.9824\n",
      "Epoch 3/15\n",
      "361/361 [==============================] - 80s 221ms/step - loss: 0.0481 - accuracy: 0.9871\n",
      "Epoch 4/15\n",
      "361/361 [==============================] - 78s 215ms/step - loss: 0.0437 - accuracy: 0.9865\n",
      "Epoch 5/15\n",
      "361/361 [==============================] - 78s 217ms/step - loss: 0.0351 - accuracy: 0.9896\n",
      "Epoch 6/15\n",
      "361/361 [==============================] - 78s 216ms/step - loss: 0.0289 - accuracy: 0.9923\n",
      "Epoch 7/15\n",
      "361/361 [==============================] - 79s 217ms/step - loss: 0.0261 - accuracy: 0.9932\n",
      "Epoch 8/15\n",
      "361/361 [==============================] - 79s 218ms/step - loss: 0.0295 - accuracy: 0.9918\n",
      "Epoch 9/15\n",
      "361/361 [==============================] - 78s 215ms/step - loss: 0.0243 - accuracy: 0.9938\n",
      "Epoch 10/15\n",
      "361/361 [==============================] - 78s 217ms/step - loss: 0.0210 - accuracy: 0.9952\n",
      "Epoch 11/15\n",
      "361/361 [==============================] - 78s 216ms/step - loss: 0.0180 - accuracy: 0.9965\n",
      "Epoch 12/15\n",
      "361/361 [==============================] - 78s 216ms/step - loss: 0.0156 - accuracy: 0.9971\n",
      "Epoch 13/15\n",
      "361/361 [==============================] - 78s 215ms/step - loss: 0.0145 - accuracy: 0.9977\n",
      "Epoch 14/15\n",
      "361/361 [==============================] - 78s 216ms/step - loss: 0.0134 - accuracy: 0.9981\n",
      "Epoch 15/15\n",
      "361/361 [==============================] - 77s 214ms/step - loss: 0.0131 - accuracy: 0.9982\n"
     ]
    }
   ],
   "source": [
    "history = tn_model.fit(train_dataset,\n",
    "                       epochs=nepochs,\n",
    "                       initial_epoch=start_epoch,\n",
    "                       verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 51s 62ms/step - loss: 0.0141 - accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "tn_model.evaluate(test_dataset);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
